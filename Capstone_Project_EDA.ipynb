{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Building up Data Pipelines using Apache Airflow \n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The main goal of the capstone project is to create a Data Lake in S3 and a DWH in Redshift using Data Pipeline orchestrating tool - Apache Airflow\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import configparser\n",
    "import boto3\n",
    "from io import StringIO\n",
    "from pyspark.sql import SparkSession\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('./dwh.cfg')\n",
    "\n",
    "AWS_ACCESS_KEY = config.get('AWS', 'AWS_KEY_ID')\n",
    "AWS_SECRET = config.get('AWS','AWS_SECRET')\n",
    "AWS_REGION = config.get('AWS','REGION')\n",
    "S3_BUCKET = config.get('S3','RAW_DATA_BUCKET')\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]= AWS_ACCESS_KEY\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]= AWS_SECRET\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.2\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime, unix_timestamp, to_date, expr,\\\n",
    "                                  date_add,udf,col,avg,mean,year,month,split,lit\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.types import StringType, DateType"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1: Project Scope and Data Gathering\n",
    "\n",
    "The scope of the project is to create a Data Lake which could be accessible for the Data Scientists and a DWH which\n",
    "could be accessed by Data Analysts who are interested in providing deeper insights into US immigration. Main focus areas include\n",
    "the issued visa types and the immigrant profiles associated with it.\n",
    "\n",
    "**Data Sources from Udacity**\n",
    "- **I94 Immigration Data**: This data comes from the US National Tourism and Trade Office found [here](https://travel.trade.gov/research/reports/i94/historical/2016.html). Each report contains international visitor arrival statistics by world regions and select countries (including top 20), type of visa, mode of transportation, age groups, states visited (first intended address only), and the top ports of entry (for select countries)\n",
    "- **U.S. City Demographics Data**: This dataset contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. Dataset comes from OpenSoft found [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/).\n",
    "- **Airport Codes**: This is a simple table of airport codes and corresponding cities. The airport codes may refer to either IATA airport code, a three-letter code which is used in passenger reservation, ticketing and baggage-handling systems, or the ICAO airport code which is a four letter code used by ATC systems and for airports that do not have an IATA airport code (from wikipedia). It comes from [here](https://datahub.io/core/airport-codes#data)\n",
    "**External Data Sources**\n",
    "I have enriched the dataset using the following data sources to make the analysis more useful and more informative\n",
    "\n",
    "- **Port of Entry/Nationality Codes**: This dataset contains information about various of port of entry codes which would be used to join the data in the I94 immigration data. The nationality codes contain the country abbreviations [Port Codes; Nationality Codes And Port of Entry codes](https://fam.state.gov/fam/09FAM/09FAM010205.html)\n",
    "- **US Visa Types**: This data is extracted from the US DHS and Wikipedia which would give information on various visa types offered by US [US Non-immigrant Visa Types](https://www.dhs.gov/immigration-statistics/nonimmigrant/NonimmigrantCOA) and [US Immigrant Visa Types](https://en.m.wikipedia.org/wiki/Visa_policy_of_the_United_States#Classes_of_visas)\n",
    "- **Airline Codes**: This data source consists of airlines IATA abbreviations and the country of origin [Airline Codes](https://www.iata.org/en/about/members/airline-list?page=30&search=&ordering=Alphabetical)\n",
    "\n",
    "In the following sections I will explore various data sets needed for the project"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. Exploring the I94 Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "df_i94_sample = pd.read_csv('./data/immigration_data_sample.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_i94_sample.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Observations:** \n",
    "- The columns in the immigration data are encoded. Therefore these columns could be Foreign Keys in the Fact table\n",
    "- To create the corresponding Dimension Tables the `SAS_Label_Descriptions` file will be used to extract them into `csv` files which will be stored in `meta_data` folder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.1 Extracting meta data from i94 immigration data label Descriptions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***Extracting Raw Data***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Reading the Label Description file and extracting information which can form dimensional tables later\n",
    "with open('./data/I94_SAS_Labels_Descriptions.SAS') as i94_descritpion:\n",
    "    i94_label_content = i94_descritpion.read()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = re.search(r'value i94cntyl.*? ;', i94_label_content, re.DOTALL).group()\n",
    "text=text.split('\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i94cit_i94res = pd.DataFrame(columns=['i94_code','country_name'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i94cit_i94res['i94_code'] = [code.split('=')[0].strip() for code in text[1:]]\n",
    "i94cit_i94res['country_name'] = [code.split('=')[1].rstrip(' ;').replace(\"'\",\"\") for code in text[1:]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i94cit_i94res.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Observation:** This data can be used to map the country codes to their respective names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = re.search(r'i94prtl.*?;', i94_label_content, re.DOTALL).group()\n",
    "text=text.split('\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i94port_i94code = pd.DataFrame(columns=['i94_port_code','i94_airport_location','i94_airport_state'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i94port_i94code['i94_port_code'] = [re.search('[A-Z](?=)\\w+', code).group() for code in text[1:-1]]\n",
    "i94port_i94code['i94_airport_location'] = [re.findall('\\'(.*?),', code.split('\\t=\\t')[1])[0] if len(re.findall('\\'(.*?),', code.split('\\t=\\t')[1]))>0 else code.split('\\t=\\t')[1].strip(\" '\") \n",
    "                                           for code in text[1:-1]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i94port_i94code['i94_airport_state'] = [code.split('\\t=\\t')[1].strip(\" '\") for code in text[1:-1]]\n",
    "i94port_i94code['i94_airport_state'] = i94port_i94code['i94_airport_state'].str.split(', ')\n",
    "i94port_i94code['i94_airport_state'] = [code[-1] for code in i94port_i94code['i94_airport_state']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for code in i94port_i94code.i94_airport_state:\n",
    "    if len(code)>2 and 'Collapsed' not in code and 'No PORT Code' not in code:\n",
    "        tmp = code.split(' ')\n",
    "        if len(tmp)>1:\n",
    "            if len(tmp[0])==2 and '(BPS)' in tmp[1] or '#ARPT' in tmp[1]:\n",
    "                i94port_i94code.loc[i94port_i94code.i94_airport_state==code,'i94_airport_state'] = tmp[0]\n",
    "\n",
    "            if len(tmp[1]) == 2 and len(tmp[0])!=2:\n",
    "                i94port_i94code.loc[i94port_i94code.i94_airport_state==code,'i94_airport_state'] = tmp[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i94port_i94code.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Observations:**\n",
    "- This table can help in mapping the `i94port` from the immigration data and its `airport_location` and `state_code` in US"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i94port_i94code.to_csv('i94_meta_data/i94port_i94code.csv',index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = re.search(r'value i94addrl.*? ;', i94_label_content, re.DOTALL).group()\n",
    "text=text.split('\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i94_state_code = pd.DataFrame(columns=['i94_state_code','i94_state_name'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i94_state_code['i94_state_code'] = [code.split(\"'='\")[0].strip(\"\\t'\") for code in text[1:]]\n",
    "i94_state_code['i94_state_name'] = [code.split(\"'='\")[1].strip(\" ';\") for code in text[1:]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i94_state_code.to_csv('i94_meta_data/i94_state_code.csv',index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.2 Combining all the above steps"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "key_name=''\n",
    "for line in i94_label_content.split(\"\\n\"):\n",
    "    line = re.sub(r\"\\s+\", \" \", line)\n",
    "    if '/*' in line and '-' in line:\n",
    "        line = line.strip('/*')\n",
    "        key_name = line.split('-')[0].strip()\n",
    "        data_dict[key_name] = []\n",
    "    if '=' in line and key_name!='':\n",
    "        data_dict[key_name].append([item.strip(';').strip(\" \").replace('\\'', '').lstrip().rstrip() for item in line.split('=')])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for key in data_dict:\n",
    "    if len(data_dict[key])>0:\n",
    "        if 'CIT' in key and 'RES' in key:\n",
    "            i94cit_i94res = pd.DataFrame(data_dict[key],columns=['i94_country_code','country_name'])\n",
    "            i94cit_i94res.loc[i94cit_i94res.country_name.str.contains('MEXICO'),'country_name'] = 'MEXICO'\n",
    "            i94cit_i94res.to_csv('i94_meta_data/i94cit_i94res.csv',index=False)\n",
    "        if 'PORT' in key:\n",
    "            i94port_i94code = pd.DataFrame(data_dict[key],columns=['i94_port_code','i94_airport_location'])\n",
    "            i94port_i94code[['port_city', 'port_state']] = i94port_i94code['i94_airport_location'].str.rsplit(',', 1, expand=True)\n",
    "            i94port_i94code.loc[i94port_i94code.port_city == 'MARIPOSA AZ','port_state'] = 'AZ'\n",
    "            i94port_i94code.loc[i94port_i94code.port_city == 'MARIPOSA AZ','port_city'] = 'MARIPOSA'\n",
    "            i94port_i94code.loc[i94port_i94code.port_city == 'WASHINGTON DC','port_state'] = 'DC'\n",
    "            i94port_i94code.drop(['i94_airport_location'], axis=1, inplace=True)\n",
    "            i94port_i94code.to_csv('i94_meta_data/i94port_i94code.csv',index=False)\n",
    "        if 'MODE' in key:\n",
    "            i94mode = pd.DataFrame(data_dict[key],columns=['i94_mode_code','i94_mode'])\n",
    "            i94mode.to_csv('i94_meta_data/i94mode.csv',index=False)\n",
    "        if 'ADDR' in key:\n",
    "            i94addr = pd.DataFrame(data_dict[key],columns=['i94_state_code','i94_state_name'])\n",
    "            i94addr.to_csv('i94_meta_data/i94addr.csv',index=False)\n",
    "        if 'VISA' in key:\n",
    "            i94visa = pd.DataFrame(data_dict[key],columns=['i94_visa_code','i94_visa'])\n",
    "            i94visa.to_csv('i94_meta_data/i94visa.csv',index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_dict.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i94cit_i94res.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.3 Exploring the SAS data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!ls ../../data/18-83510-I94-Data-2016/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "def to_datetime(x):\n",
    "    try:\n",
    "        start = datetime(1960, 1, 1)\n",
    "        return start + timedelta(days=int(x))\n",
    "    except:\n",
    "        return None\n",
    "udf_to_datetime_sas = udf(lambda x: to_datetime(x), DateType())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def to_datetime_frm_str(x):\n",
    "    try:\n",
    "        if x != 'D/S':\n",
    "            return datetime.strptime(x, '%m%d%Y')\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "udf_to_datetime_frm_str = udf(lambda x: to_datetime_frm_str(x), DateType())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_i94 =spark.read.format('com.github.saurfang.sas.spark').option(\"dateFormat\", \"yyyyMMdd\").option(\"inferSchema\", \"true\").\\\n",
    "                 load('../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat').withColumn(\"arrival_date\", lit(udf_to_datetime_sas(\"arrdate\")))\\\n",
    "                .withColumn(\"departure_date\", lit(udf_to_datetime_sas(\"depdate\")))\\\n",
    "                .withColumn(\"departure_deadline\", lit(udf_to_datetime_frm_str(\"dtaddto\"))) \\\n",
    "                .withColumn(\"month_year\", lit('jan'+'_'+'16'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Printing shape of the dataset\n",
    "print(\"Number of Columns: {}\".format(len(df_i94.columns)))\n",
    "print(\"Number of Rows: {}\".format(df_i94.count()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_i94.limit(15).toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_i94.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_i94 = df_i94.drop('validres','delete_days','delete_mexl','delete_dup','delete_recdup','depdate','delete_visa','arrdate','dtadfile','dtaddto' ,'occup', 'entdepa', 'entdepd', 'entdepu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_cast_select_exprs(sas_cols, schema_cols):\n",
    "    if sas_cols != '':\n",
    "        exprs = [\"{} AS {}\".format(dfc,sc) for dfc, sc in zip(sas_cols, schema_cols)]\n",
    "    else:\n",
    "        raise ValueError('Cannot create Select Expression without proper header')\n",
    "    return exprs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sas_columns = ['cast(cicid as int)','cast(i94yr as int)','cast(i94mon as int)','cast(i94cit as int)',\n",
    "\t\t\t   'cast(i94res as int)','i94port','arrival_date','cast(i94mode as int)',\n",
    "\t\t\t   'i94addr','departure_date','departure_deadline','cast(i94bir as int)','cast(i94visa as int)',\n",
    "\t\t\t   'cast(count as int)','visapost','matflag','cast(biryear as int)',\n",
    "\t\t\t   'gender','insnum','airline','cast(admnum as float)','fltno','visatype',\"month_year\"]\n",
    "\n",
    "schema_columns = ['cicid','entry_year','entry_month','country_id','res_id','port_id','arrival_date',\n",
    "\t\t\t\t  'mode_id','state_code','departure_date','departure_deadline','age','visa_reason_id','count','visa_post',\n",
    "\t\t\t\t  'matched_flag','birth_year','gender','ins_num','airline_abbr','admission_num','flight_no','visa_type','month_year']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_i94=df_i94.selectExpr(create_cast_select_exprs(sas_columns,schema_columns))\n",
    "df_i94.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_i94.limit(5).toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# storing the data in S3 for later use\n",
    "%%time\n",
    "for file in os.listdir('../../data/18-83510-I94-Data-2016/'):\n",
    "    path='../../data/18-83510-I94-Data-2016/{}'.format(file)\n",
    "    df_i94 = spark.read.format('com.github.saurfang.sas.spark').option(\"inferSchema\", \"true\").option(\"dateFormat\", \"yyyyMMdd\").load(path)\\\n",
    "             .withColumn(\"arrival_date\", udf_to_datetime_sas(\"arrdate\")) \\\n",
    "             .withColumn(\"departure_date\", udf_to_datetime_sas(\"depdate\")).withColumn(\"departure_deadline\", udf_to_datetimefrstr(\"dtaddto\"))\n",
    "    df_i94=df_i94.drop('validres','delete_days','delete_mexl','delete_dup','delete_recdup','delete_visa','arrdate','dtadfile', 'occup', 'entdepa', 'entdepd', 'entdepu')\n",
    "    bucket_path = r\"s3a://{}/i94_raw_data/{}_sasdata.parquet\".format(S3_BUCKET,file.strip('.sas7bdat').split('_')[1])\n",
    "    print(bucket_path)\n",
    "    df_i94.write.parquet(bucket_path, \"overwrite\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "for file in os.listdir('../../data/18-83510-I94-Data-2016/'):\n",
    "    path='../../data/18-83510-I94-Data-2016/{}'.format(file)\n",
    "    print(path)\n",
    "    df_i94 = spark.read.format('com.github.saurfang.sas.spark').option(\"inferSchema\", \"true\").option(\"dateFormat\", \"yyyyMMdd\").load(path)\n",
    "    df_i94=df_i94.drop('validres','delete_days','delete_mexl','delete_dup','delete_recdup','delete_visa','arrdate','dtadfile', 'occup', 'entdepa', 'entdepd', 'entdepu')\n",
    "    df_i94_total = df_i94.union(df_i94)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Printing shape of the combined dataset shape\n",
    "print(\"Number of Columns: {}\".format(len(df_i94_total.columns)))\n",
    "print(\"Number of Rows: {}\".format(df_i94_total.count()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Observations:** From the dataset and the SAS_Label description file:\n",
    "- The column `cicid` is like uuid to me\n",
    "- The columns `validres, delete_days, delete_mexl, delete_dup, delete_recdup, delete_visa` are additional columns and can be removed by inferring from the sample csv file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.4 Exploring the Airport Codes Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_spark = {} # using this dictionary for plotting in section 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "def parse_latitude(x):\n",
    "    y = x.strip().split(',')\n",
    "    return float(y[1])\n",
    "\n",
    "\n",
    "def parse_longitude(x):\n",
    "    y = x.strip().split(',')\n",
    "    return float(y[0])\n",
    "\n",
    "\n",
    "def port_of_entry(x):\n",
    "    return x.strip().split(', ')[0]\n",
    "\n",
    "\n",
    "def parse_state_code(x):\n",
    "    return x.strip().split('-')[-1]\n",
    "\n",
    "\n",
    "def parse_country_code(x):\n",
    "    return x.strip().split('-')[0]\n",
    "\n",
    "\n",
    "udf_parse_port_of_entry = udf(lambda x: port_of_entry(x), StringType())\n",
    "udf_parse_latitude = udf(lambda x: parse_latitude(x), FloatType())\n",
    "udf_parse_longitude = udf(lambda x: parse_longitude(x), FloatType())\n",
    "udf_parse_state_code = udf(lambda x: parse_state_code(x), StringType())\n",
    "udf_parse_country_code = udf(lambda x: parse_country_code(x), StringType())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_airport = spark.read.format('csv').options(header='true', inferSchema='true').load('./data/airport-codes.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_airport.limit(10).toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_airport.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_airport = df_airport.withColumn(\"airport_latitude\", udf_parse_latitude(\"coordinates\"))\\\n",
    ".withColumn(\"airport_longitude\", udf_parse_longitude(\"coordinates\"))\\\n",
    ".withColumn(\"country\", udf_parse_country_code(\"iso_region\"))\\\n",
    ".withColumn(\"state_code\", udf_parse_state_code(\"iso_region\"))\\\n",
    ".withColumnRenamed(\"ident\", \"icao_code\")\\\n",
    ".withColumnRenamed(\"nearest_city\", \"nearest_city\")\\"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_airport=df_airport.drop(\"coordinates\", \"gps_code\", \"local_code\",\"iso_region\", \"iso_country\")\n",
    "df_airport.limit(10).toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Printing Shape of the Dataset\n",
    "print(\"Number of Columns: {}\".format(len(df_airport.columns)))\n",
    "print(\"Number of Rows: {}\".format(df_airport.count()))\n",
    "\n",
    "df_airport=df_airport.drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_airport.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_airport.summary(\"count\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_spark['airport-codes'] = df_airport"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.5 Exploring the US cities Demographics Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_demographics = spark.read.format('csv').options(header='true', inferSchema='true',delimiter=';').load('./data/us-cities-demographics.csv')\\\n",
    "                .withColumnRenamed(\"State\", \"state\")\\\n",
    "                .withColumnRenamed(\"State Code\", \"state_code\")\\\n",
    "                .withColumnRenamed(\"City\", \"city\")\\\n",
    "                .withColumnRenamed(\"Median Age\", \"median_age\")\\\n",
    "                .withColumnRenamed(\"Male Population\", \"male_population\")\\\n",
    "                .withColumnRenamed(\"Female Population\", \"female_population\")\\\n",
    "                .withColumnRenamed(\"Total Population\", \"total_population\")\\\n",
    "                .withColumnRenamed(\"Number of Veterans\", \"num_of_veterans\")\\\n",
    "                .withColumnRenamed(\"Foreign-born\", \"foreign_born\")\\\n",
    "                .withColumnRenamed(\"Average Household Size\", \"avg_household_size\")\\\n",
    "                .withColumnRenamed(\"Race\", \"predominant_race\")\\\n",
    "                .withColumnRenamed(\"Count\", \"count\")\\"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Printing Shape of the Dataset\n",
    "print(\"Number of Columns: {}\".format(len(df_demographics.columns)))\n",
    "print(\"Number of Rows: {}\".format(df_demographics.count()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_demographics=df_demographics.drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_demographics.limit(10).toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_demographics.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Printing Shape of the Dataset after dropping duplicates\n",
    "print(\"Number of Columns: {}\".format(len(df_demographics.columns)))\n",
    "print(\"Number of Rows: {}\".format(df_demographics.count()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_demographics.summary(\"count\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_demographics.where(df_demographics.male_population.isNull()).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_spark['us-cities-demographics'] = df_demographics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.6 Exploring I94 labels data\n",
    "\n",
    "**checking i94 meta_data**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i94_addr = spark.read.format('csv').load('i94_meta_data/i94addr.csv', header=True, inferSchema=True)\n",
    "i94_addr.limit(5).toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i94_addr_df = i94_addr.selectExpr(\"i94_state_code as state_code\",\"i94_state_name as state_name\")\n",
    "i94_addr_df.summary(\"count\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i94_cit = spark.read.format('csv').load('i94_meta_data/i94cit_i94res.csv', header=True, inferSchema=True)\n",
    "i94_cit.limit(5).toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i94_cit_df = i94_cit.selectExpr(\"i94_country_code as country_id\",\"country_name as country\")\n",
    "i94_cit_df.summary(\"count\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i94_mode = spark.read.format('csv').load('i94_meta_data/i94mode.csv', header=True, inferSchema=True)\n",
    "i94_mode_df = i94_mode.selectExpr(\"i94_mode_code as mode_id\",\"i94_mode as transportation_mode\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i94_mode_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i94_port = spark.read.format('csv').load('i94_meta_data/i94port_i94code.csv', header=True, inferSchema=True)\n",
    "i94_port.limit(5).toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i94_port.filter(i94_port.port_state.contains('BPS')).show(20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i94_port.filter(i94_port.port_state.contains('#ARPT')).show(20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parse_state_code(x):\n",
    "    if x is not None and 'BPS' in x:\n",
    "        return x.strip().split('(BPS)')[0].strip()\n",
    "    elif x is not None and 'ARPT' in x:\n",
    "        return x.strip().split('#ARPT')[0].strip()\n",
    "    elif x is not None and 'SECTOR HQ' in x:\n",
    "        return x.strip().split('(BP - SECTOR HQ)')[0].strip()\n",
    "    elif x is not None and 'INTL' in x:\n",
    "        return x.strip().split('#INTL')[0].strip()\n",
    "    else:\n",
    "        return x\n",
    "udf_parse_state_code = udf(lambda x: parse_state_code(x), StringType())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i94_port=i94_port.withColumn(\"port_state_cleaned\", udf_parse_state_code(\"port_state\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i94_port.filter(i94_port.port_state.contains('BPS')).show(20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i94_port.filter(i94_port.port_state.contains('ARPT')).show(20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i94_port_df = i94_port.selectExpr(\"i94_port_code as port_code\",\"port_city as city\",\"port_state_cleaned as state_code_or_country\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i94_port_df.show(20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_spark['port-of-entry-codes'] = i94_port_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i94_visa = spark.read.format('csv').load('i94_meta_data/i94visa.csv', header=True, inferSchema=True)\n",
    "i94_visa.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i94_visa_df = i94_visa.selectExpr(\"i94_visa_code as visa_code\",\"i94_visa as visa_purpose\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i94_visa_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "visa = spark.read.format('csv').load('./data/visa-type.csv', header=True, inferSchema=True)\n",
    "visa.show(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "visa_df = visa.withColumnRenamed(\"visa-type\", \"visa_type\").withColumnRenamed(\"description\", \"visa_type_description\")\n",
    "visa_df.limit(10).toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_spark['visa-type']=visa_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "visa_port = spark.read.format('csv').load('./data/visa-issuing-ports.csv', header=True, inferSchema=True)\n",
    "visa_port.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "visa_port_df = visa_port.selectExpr(\"Post as port_of_issue\",\"Code as visa_post_code\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "visa_port_df.limit(10).toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_spark['visa-issue-ports']=visa_port_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "port_of_entry = spark.read.format('csv').load('./data/port-of-entry-codes.csv', header=True, inferSchema=True)\n",
    "port_of_entry.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note: I have used the port of entry data from I94 SAS labels**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.7 Exploring Airlines data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "airlines = spark.read.format('csv')\\\n",
    "    .option(\"delimiter\", \"\\t\")\\\n",
    "    .load('./data/airlines-codes.csv', header=True, inferSchema=True)\\\n",
    "    .withColumnRenamed(\"AIRLINE NAME\", \"airline_name\")\\\n",
    "    .withColumnRenamed(\"IATA DESIGNATOR\", \"airline_iata_code\")\\\n",
    "    .withColumnRenamed(\"3 DIGIT CODE\", \"airline_3_digit_code\")\\\n",
    "    .withColumnRenamed(\"ICAO CODE\", \"airline_icao_code\")\\\n",
    "    .withColumnRenamed(\"COUNTRY / TERRITORY\", \"origin_country\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "airlines.limit(10).toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Printing Shape of the Dataset after dropping duplicates\n",
    "airlines=airlines.drop_duplicates(['airline_iata_code'])\n",
    "print(\"Number of Columns: {}\".format(len(airlines.columns)))\n",
    "print(\"Number of Rows: {}\".format(airlines.count()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "airlines.summary(\"count\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_spark['airlines-codes'] = airlines"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Exploring data quality issues, like missing values, duplicate data, etc.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(15,8)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_files = ['airlines-codes','airport-codes','port-of-entry-codes','us-cities-demographics','visa-issue-ports','visa-type']\n",
    "\n",
    "for file in data_files:\n",
    "    df = data_spark[file]\n",
    "    null_count=df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).toPandas().T.reset_index()\n",
    "    null_count.columns = ['col','null_counts']\n",
    "    null_count['%_null_counts']=null_count['null_counts']/(df.count())\n",
    "    if (null_count['%_null_counts']>0).any():\n",
    "        plt.figure(figsize=(15,8))\n",
    "        sns.barplot(x='col',y='%_null_counts',data=null_count)\n",
    "        plt.title(file)\n",
    "        plt.xticks(rotation=90)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Observations:**\n",
    "- In demographics data the % percentage of null values in the data set is very small, these rows can be dropped\n",
    "- In airlines data the `airline_3_digit_code` columns has a very few number of null values, not needed to drop rows there. Can be a choice\n",
    "- In airports data the `iata_code` column has high number of null values. this column can be dropped\n",
    "- In port of entry codes data the `state_code_or_country` column has some null values, but this is because the data is not having any codes provided"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Cleaning Steps\n",
    "All the necessary steps I have provided in the above sections"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3: Project Architecture and DWH Data Model\n",
    "The following architecture is used in the project\n",
    "![Project_Architecture](./AWS_Help/architechture.png)\n",
    "#### 3.1 Conceptual Data Model\n",
    "I used a Star schema to build the DWH in redshift and is as follows:\n",
    "![Schema](./AWS_Help/data_model.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "To create the Data Lake and DWH the following steps are to be followed:\n",
    "\n",
    "1. Create AWS account and fill the `dwh.cfg` file with necessary details\n",
    "2. Use the data in the `data` folder and upload them into S3 bucket using the `create_resources.py` file\n",
    "3. Copy the `dag` , `emr_bootstrap` folder and `plugins` folder to your `AIRFLOW` directory\n",
    "4. Once the data is in place run the `capstone_meta_data_dag` to fill up a new S3 bucket which is used as a **`staging area`**\n",
    "5. Run the `capstone_immigration_dag` to fill up the data lake in S3 with the SAS data\n",
    "6. Run the `capstone_dwh_dag` to fill up the DWH in redshift"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note: All the codes are available in the dags folder** "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.1.1 capstone_meta_data_dag \n",
    "\n",
    "**Note:** Run when the data is refreshed not needed to run every day, can be monthly or by trigger\n",
    "**Action:** Uses the Data stored in a S3 bucket and performs all the regex operations for the I94 Labels data and other transforms needed for other data and stores the data in parquet format in a different S3 bucket which is used as staging area\n",
    "\n",
    "![meta_data_dag](./AWS_Help/metadata_dag.PNG)\n",
    "\n",
    "\n",
    "#### 4.1.2 capstone_immigration_dag \n",
    "\n",
    "**Note:** Run when the data is added monthly can be daily if the data is provided daily\n",
    "**Action:** Uses the SAS Data stored in a S3 bucket and performs all the transforms needed for the data and stores it in parquet format in a different S3 bucket which is used as staging area\n",
    "\n",
    "![meta_data_dag](./AWS_Help/immigration_data_transform.png)\n",
    "\n",
    "\n",
    "#### 4.1.3 capstone_DWH_dag \n",
    "\n",
    "**Note:** Run when the data is added monthly can be daily if the data is provided daily\n",
    "**Action:** Uses the SAS Data and other meta data stored in a staging S3 bucket and performs copy operations to AWS redshift and the data model is contrained accordingly\n",
    "\n",
    "![meta_data_dag](./AWS_Help/dwh_dag.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "From the data model it can be observed that the DWH tables have key constraints defined accordingly with data type and primary key when needed\n",
    "\n",
    "Data Quality checks were included:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Extras"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### move files to S3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import configparser\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('./dwh.cfg')\n",
    "\n",
    "AWS_ACCESS_KEY = config.get('AWS', 'AWS_KEY_ID')\n",
    "AWS_SECRET = config.get('AWS','AWS_SECRET')\n",
    "AWS_REGION = config.get('AWS','REGION')\n",
    "S3_BUCKET = config.get('S3','BUCKET_NAME')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3',region_name=AWS_REGION,\n",
    "                    aws_access_key_id=AWS_ACCESS_KEY,\n",
    "                    aws_secret_access_key=AWS_SECRET)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "csv_buffer = StringIO()\n",
    "df_airports = pd.read_csv('airport-codes_csv.csv')\n",
    "df_airports.to_csv(csv_buffer)\n",
    "s3.Object(S3_BUCKET,'airport_codes/airport-codes_csv.csv').put(Body=csv_buffer.getvalue())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_demographics = pd.read_csv('us-cities-demographics.csv')\n",
    "df_demographics.to_csv(csv_buffer)\n",
    "s3.Object(S3_BUCKET,'us-demographics/us-cities-demographics.csv').put(Body=csv_buffer.getvalue())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "out_buffer = BytesIO()\n",
    "df_i94.to_parquet(out_buffer, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}